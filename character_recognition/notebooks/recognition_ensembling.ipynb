{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning"
      ],
      "metadata": {
        "id": "8qyrqjT_gCkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c9LJgmVHlTOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b4d665-5c61-4e01-f3cc-1e3613f06eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")\n",
        "\n",
        "from pathlib import Path\n",
        "data_path = Path(\"/content/gdrive/MyDrive/ragaik/sample_ensembling_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import re\n",
        "\n",
        "from typing import Dict, List, Tuple, Union\n",
        "\n",
        "\n",
        "class LaTeXTokenizer:\n",
        "    def __init__(self):\n",
        "        self.special_tokens = [\"[PAD]\", \"[BOS]\", \"[EOS]\", \"[UNK]\"]\n",
        "        self.vocab = {}\n",
        "        self.token_to_id = {}\n",
        "        self.id_to_token = {}\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        # Tokenize LaTeX using regex to capture commands, numbers and other characters\n",
        "        return re.findall(r\"\\\\[a-zA-Z]+|\\\\.|[a-zA-Z0-9]|\\S\", text)\n",
        "\n",
        "    def build_vocab(self, texts: List[str]):\n",
        "        # Add special tokens to vocabulary\n",
        "        for token in self.special_tokens:\n",
        "            self.vocab[token] = len(self.vocab)\n",
        "\n",
        "        # Create a counter to hold token frequencies\n",
        "        counter = collections.Counter()\n",
        "\n",
        "        # Tokenize each text and update the counter\n",
        "        for text in texts:\n",
        "            tokens = self.tokenize(text)\n",
        "            counter.update(tokens)\n",
        "\n",
        "        # Add tokens to vocab based on their frequency\n",
        "        for token, _ in counter.most_common():\n",
        "            if token not in self.vocab:\n",
        "                self.vocab[token] = len(self.vocab)\n",
        "\n",
        "        # Build dictionaries for token to ID and ID to token conversion\n",
        "        self.token_to_id = self.vocab\n",
        "        self.id_to_token = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # Tokenize the input text and add start and end tokens\n",
        "        tokens = [\"[BOS]\"] + self.tokenize(text) + [\"[EOS]\"]\n",
        "\n",
        "        # Map tokens to their IDs, using [UNK] for unknown tokens\n",
        "        unk_id = self.token_to_id[\"[UNK]\"]\n",
        "        return [self.token_to_id.get(token, unk_id) for token in tokens]\n",
        "\n",
        "    def decode(self, token_ids: List[int]) -> List[str]:\n",
        "        # Map token IDs back to tokens\n",
        "        tokens = [self.id_to_token.get(id, \"[UNK]\") for id in token_ids]\n",
        "\n",
        "        # Remove tokens beyond the [EOS] token\n",
        "        if \"[EOS]\" in tokens:\n",
        "            tokens = tokens[: tokens.index(\"[EOS]\")]\n",
        "\n",
        "        # Replace [UNK] with ?\n",
        "        tokens = [\"?\" if token == \"[UNK]\" else token for token in tokens]\n",
        "\n",
        "        # Reconstruct the original text, ignoring special tokens\n",
        "        return \"\".join([token for token in tokens if token not in self.special_tokens])"
      ],
      "metadata": {
        "id": "0-2j6OBQgB3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class PositionalEncoding1D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        dropout: float = 0.1,\n",
        "        max_len: int = 1000,\n",
        "        temperature: float = 10000.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Generate position and dimension tensors for encoding\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        dim_t = torch.arange(0, d_model, 2)\n",
        "        div_term = torch.exp(dim_t * (-math.log(temperature) / d_model))\n",
        "\n",
        "        # Initialize and fill the positional encoding matrix with sine/cosine values\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, sequence_length, d_model = x.shape\n",
        "        return self.dropout(x + self.pe[None, :sequence_length, :])\n",
        "\n",
        "\n",
        "class PositionalEncoding2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        dropout: float = 0.1,\n",
        "        max_len: int = 30,\n",
        "        temperature: float = 10000.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Generate position and dimension tensors for 1D encoding\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        dim_t = torch.arange(0, d_model, 2)\n",
        "        div_term = torch.exp(dim_t * (-math.log(temperature) / d_model))\n",
        "\n",
        "        # Initialize and fill the 1D positional encoding matrix with sine/cosine values\n",
        "        pe_1D = torch.zeros(max_len, d_model)\n",
        "        pe_1D[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe_1D[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Compute the 2D positional encoding matrix using outer product\n",
        "        pe_2D = torch.zeros(max_len, max_len, d_model)\n",
        "        for i in range(d_model):\n",
        "            pe_2D[:, :, i] = pe_1D[:, i].unsqueeze(-1) + pe_1D[:, i].unsqueeze(0)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"pe\", pe_2D)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, height, width, d_model = x.shape\n",
        "        return self.dropout(x + self.pe[None, :height, :width, :])"
      ],
      "metadata": {
        "id": "oZm0K0CJgbj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightning.pytorch as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch import nn, optim, Tensor\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils import data\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "BzALY3QnfxXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import densenet121, DenseNet121_Weights\n",
        "\n",
        "\n",
        "class Permute(nn.Module):\n",
        "    def __init__(self, *dims: int):\n",
        "        super().__init__()\n",
        "        self.dims = dims\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.permute(*self.dims)\n",
        "\n",
        "\n",
        "class Model(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int,\n",
        "        nhead: int,\n",
        "        dim_feedforward: int,\n",
        "        dropout: float,\n",
        "        num_layers: int,\n",
        "        lr: float = 1e-4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.example_input_array = (\n",
        "            torch.rand(16, 3, 384, 512),  # batch x channel x height x width\n",
        "            torch.ones(16, 64, dtype=torch.long),  # batch x sequence length\n",
        "            torch.zeros(64, 64),  # sequence length x sequence length\n",
        "        )\n",
        "\n",
        "        # Define the encoder architecture\n",
        "        densenet = densenet121(weights=DenseNet121_Weights.DEFAULT)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Sequential(*list(densenet.children())[:-1]),  # remove the final layer\n",
        "            nn.Conv2d(1024, d_model, kernel_size=1),\n",
        "            Permute(0, 2, 3, 1),\n",
        "            PositionalEncoding2D(d_model, dropout),\n",
        "            nn.Flatten(1, 2),\n",
        "        )\n",
        "\n",
        "        # Define the decoder architecture\n",
        "        self.tgt_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.word_positional_encoding = PositionalEncoding1D(d_model, dropout)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(\n",
        "                d_model, nhead, dim_feedforward, dropout, batch_first=True\n",
        "            ),\n",
        "            num_layers,\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def decoder(self, features, tgt, tgt_mask):\n",
        "        padding_mask = tgt.eq(0)\n",
        "        tgt = self.tgt_embedding(tgt) * math.sqrt(self.hparams.d_model)\n",
        "        tgt = self.word_positional_encoding(tgt)\n",
        "        tgt = self.transformer_decoder(\n",
        "            tgt, features, tgt_mask=tgt_mask, tgt_key_padding_mask=padding_mask\n",
        "        )\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "    def forward(self, src, tgt, tgt_mask):\n",
        "        features = self.encoder(src)\n",
        "        output = self.decoder(features, tgt, tgt_mask)\n",
        "        return output\n",
        "\n",
        "    def beam_search(\n",
        "        self,\n",
        "        src,\n",
        "        tokenizer,\n",
        "        max_seq_len: int = 256,\n",
        "        beam_width: int = 3,\n",
        "    ) -> List[str]:\n",
        "        with torch.no_grad():\n",
        "            batch_size = src.size(0)\n",
        "            vocab_size = self.hparams.vocab_size\n",
        "            features = self.encoder(src).detach()\n",
        "            features_rep = features.repeat_interleave(beam_width, dim=0)\n",
        "            tgt_mask = torch.triu(\n",
        "                torch.ones(max_seq_len, max_seq_len) * float(\"-inf\"), diagonal=1\n",
        "            ).to(src.device)\n",
        "\n",
        "            # Initialize with [BOS]\n",
        "            beams = torch.ones(batch_size, 1, 1).long().to(src.device)\n",
        "\n",
        "            # Handle first step separately\n",
        "            output = self.decoder(features, beams[:, 0, :], tgt_mask[:1, :1])\n",
        "            next_probs = output[:, -1, :].log_softmax(dim=-1)\n",
        "            beam_scores, indices = next_probs.topk(beam_width, dim=-1)\n",
        "            beams = torch.cat(\n",
        "                [beams.repeat_interleave(beam_width, dim=1), indices.unsqueeze(2)],\n",
        "                dim=-1,\n",
        "            )\n",
        "\n",
        "            for i in range(2, max_seq_len):\n",
        "                tgt = beams.view(batch_size * beam_width, i)\n",
        "                output = self.decoder(features_rep, tgt, tgt_mask[:i, :i])\n",
        "                next_probs = output[:, -1, :].log_softmax(dim=-1)\n",
        "\n",
        "                next_probs += beam_scores.view(batch_size * beam_width, 1)\n",
        "                next_probs = next_probs.view(batch_size, -1)\n",
        "\n",
        "                beam_scores, indices = next_probs.topk(beam_width, dim=-1)\n",
        "                beams = torch.cat(\n",
        "                    [\n",
        "                        beams[\n",
        "                            torch.arange(batch_size).unsqueeze(-1),\n",
        "                            indices // vocab_size,\n",
        "                        ],\n",
        "                        (indices % vocab_size).unsqueeze(2),\n",
        "                    ],\n",
        "                    dim=-1,\n",
        "                )\n",
        "\n",
        "        best_beams = beams[:, 0, :]  # taking the best beam for each batch\n",
        "        return [tokenizer.decode(seq.tolist()) for seq in best_beams]"
      ],
      "metadata": {
        "id": "7nkGlZ5ZeYhq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}