{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6caab5",
   "metadata": {},
   "source": [
    "# –ß—Ç–æ–±—ã —Ä–∞–±–æ—Ç–∞–ª–æ –∑–∞—Å—É–Ω—å—Ç–µ –≤ –ø–∞–ø–∫—É —Å —Å–∫—Ä–∏–ø—Ç–∞–º–∏ :)))))))))))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "37899e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports & constants\n",
    "import json, cv2, torch, numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "CKPT_DIR   = Path(\"../../../checkpoints\")\n",
    "CKPT_PATH = CKPT_DIR / \"epoch=54-step=3630.ckpt\"      \n",
    "TOKEN_PATH = CKPT_DIR / \"tokenizer.pkl\"                \n",
    "\n",
    "def load_rgb(path: str | Path) -> np.ndarray:\n",
    "    '''\n",
    "    cv2.imread —á–∏—Ç–∞–µ—Ç –≤ BGR, –∞ –≤—Å–µ –º–æ–¥–µ–ª–∏ –∂–¥—É—Ç RGB.\n",
    "    '''\n",
    "    return cv2.imread(str(path), cv2.IMREAD_COLOR)[:, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "71f59d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "resize_target = (384, 512)           # H, W\n",
    "\n",
    "def preprocess_formula(img: np.ndarray):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    thr  = cv2.adaptiveThreshold(gray, 255,\n",
    "                                 cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                 cv2.THRESH_BINARY_INV, 31, 15)\n",
    "    thr  = cv2.dilate(thr, np.ones((2, 2), np.uint8), 1)\n",
    "\n",
    "    h, w = thr.shape\n",
    "    scale = min(384 / h, 512 / w)\n",
    "    new_h, new_w = int(h * scale), int(w * scale)\n",
    "    thr = cv2.resize(thr, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    canvas = np.full((384, 512), 255, np.uint8)\n",
    "    y0, x0 = (384 - new_h) // 2, (512 - new_w) // 2\n",
    "    canvas[y0:y0 + new_h, x0:x0 + new_w] = thr\n",
    "\n",
    "    canvas_rgb = cv2.cvtColor(canvas, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    tensor = T.ToTensor()(Image.fromarray(canvas_rgb)).unsqueeze(0)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a458bdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ –ò—Å–ø–æ–ª—å–∑—É—é —Ä–æ–¥–Ω—É—é LatexOCR —Å —á–µ–∫-–ø–æ–π–Ω—Ç–æ–º.\n"
     ]
    }
   ],
   "source": [
    "# %% –§–æ—Ä–º—É–ª—å–Ω–∞—è OCR (LatexOCR)\n",
    "import sys, pickle, traceback, torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from latex_ocr_model import Model, LaTeXTokenizer\n",
    "\n",
    "resize_norm = T.Compose([T.ToTensor(), T.Resize((384, 512))])\n",
    "\n",
    "# —á—Ç–æ–±—ã pickle –Ω–∞—à—ë–ª –∫–ª–∞—Å—Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, ¬´–ø—Ä–æ–∫–∏–¥—ã–≤–∞–µ–º¬ª –µ–≥–æ –≤ __main__\n",
    "sys.modules[\"__main__\"].LaTeXTokenizer = LaTeXTokenizer\n",
    "tokenizer = (pickle.load(open(TOKEN_PATH, \"rb\"))\n",
    "             if TOKEN_PATH.exists() else LaTeXTokenizer())\n",
    "\n",
    "def build_formula_ocr():\n",
    "    if CKPT_PATH.exists():\n",
    "        try:\n",
    "            ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "            h = ckpt[\"hyper_parameters\"]          # dict —Å–æ –≤—Å–µ–º–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏\n",
    "            model = Model(\n",
    "                vocab_size      = h[\"vocab_size\"],\n",
    "                d_model         = h[\"d_model\"],\n",
    "                nhead           = h[\"nhead\"],\n",
    "                dim_feedforward = h[\"dim_feedforward\"],\n",
    "                dropout         = h[\"dropout\"],\n",
    "                num_layers      = h[\"num_layers\"],\n",
    "            )\n",
    "            # ---- –≤–µ—Å–∞ -------------------------------------------------------\n",
    "            state = {k.split('.',1)[1] if k.startswith(\n",
    "                     (\"model.\",\"net.\",\"module.\")) else k : v\n",
    "                     for k,v in ckpt[\"state_dict\"].items()}\n",
    "            model.load_state_dict(state, strict=False)\n",
    "            model.eval()\n",
    "\n",
    "            def ocr_formula(img):\n",
    "                x = resize_norm(Image.fromarray(img)).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    pred = model.greedy_search(x, tokenizer)[0]\n",
    "                return pred, 0.99\n",
    "\n",
    "            print(\"–†–æ–¥–Ω–∞—è LatexOCR —Å —á–µ–∫-–ø–æ–π–Ω—Ç–æ–º.\")\n",
    "            return ocr_formula\n",
    "\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # ----- fallback: Pix2Tex ---------------------------------------------------\n",
    "    from pix2tex.cli import LatexOCR as _Pix2Tex\n",
    "    print(\"–ß–µ–∫-–ø–æ–π–Ω—Ç –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è ‚Äî –ø–µ—Ä–µ—à—ë–ª –Ω–∞ Pix2Tex.\")\n",
    "    fallback_model = _Pix2Tex()\n",
    "\n",
    "    def ocr_formula(img):\n",
    "        return fallback_model(img), 0.99\n",
    "\n",
    "    return ocr_formula\n",
    "\n",
    "ocr_formula = build_formula_ocr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8739e788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# %% –†—É—Å—Å–∫–∏–π —Ä—É–∫–æ–ø–∏—Å–Ω—ã–π OCR  (TroCR¬†RU¬†/ HF¬†pipeline)\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "ru_pipe = pipeline(\"image-to-text\", model=\"raxtemur/trocr-base-ru\", device=-1)  # CPU\n",
    "\n",
    "def ocr_russian(img: np.ndarray) -> Tuple[str, float]:\n",
    "    '''\n",
    "    :param img: RGB numpy array (H, W, 3)\n",
    "    :return: (decoded_text, confidence_stub)\n",
    "    '''\n",
    "    txt = ru_pipe(Image.fromarray(img))[0][\"generated_text\"]\n",
    "    return txt, 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "82b5bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è YOLO ‚Äî text boxes\n",
    "from detection_generator import TextBoxDetector   # our new library\n",
    "\n",
    "# instantiate once so we avoid re-loading weights for every frame\n",
    "detector = TextBoxDetector(\n",
    "    weights=\"../../../checkpoints/best.pt\",            \n",
    "    conf=0.25,                                  \n",
    "    iou=0.45,\n",
    ")\n",
    "\n",
    "def find_boxes(img):\n",
    "    \"\"\"\n",
    "    Returns a list of (x1, y1, x2, y2) tuples in *image* pixel coordinates.\n",
    "    The rest of the notebook expects this exact format.\n",
    "    \"\"\"\n",
    "    detections = detector(img)                  # list[dict] from the library\n",
    "    return [tuple(map(int, det[\"bbox\"])) for det in detections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c2cf3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_boxes(img, detections):\n",
    "    routed = []\n",
    "    for det in detections:\n",
    "        # --- –∏–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã ------------------------------------------\n",
    "        if isinstance(det, dict):                     # –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä–∞\n",
    "            x1, y1, x2, y2 = map(int, det[\"bbox\"])\n",
    "            label = det.get(\"class_name\")             # –º–æ–∂–µ—Ç –±—ã—Ç—å None\n",
    "        else:                                         # —Å—Ç–∞—Ä—ã–π –∫–æ—Ä—Ç–µ–∂ (x,y,w,h)\n",
    "            x, y, w, h = map(int, det)\n",
    "            x1, y1, x2, y2 = x, y, x + w, y + h\n",
    "            label = None\n",
    "\n",
    "        patch = img[y1:y2, x1:x2]\n",
    "\n",
    "        # --- –≤—ã–±–∏—Ä–∞–µ–º OCR ---------------------------------------------------\n",
    "        if label == \"formula\":\n",
    "            txt, conf = ocr_formula(patch)\n",
    "            kind = \"formula\"\n",
    "        elif label == \"text\":\n",
    "            txt, conf = ocr_russian(patch)\n",
    "            kind = \"text\"\n",
    "        else:\n",
    "            # fallback-—ç–≤—Ä–∏—Å—Ç–∏–∫–∞, –µ—Å–ª–∏ –º–µ—Ç–∫–∏ –Ω–µ—Ç\n",
    "            f_txt, _ = ocr_formula(patch)\n",
    "            r_txt, _ = ocr_russian(patch)\n",
    "            if re.search(r\"[–ê-–Ø–∞-—è–Å—ë]\", r_txt):\n",
    "                txt, conf, kind = r_txt, 0.99, \"text\"\n",
    "            else:\n",
    "                txt, conf, kind = f_txt, 0.99, \"formula\"\n",
    "\n",
    "        routed.append({\n",
    "            \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
    "            \"type\": kind,\n",
    "            \"text\": txt,\n",
    "            \"conf\": float(conf),\n",
    "        })\n",
    "    return routed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3f7eb636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "def process_image(path: str | Path) -> List[Dict]:\n",
    "    img   = load_rgb(path)\n",
    "    boxes = find_boxes(img)\n",
    "    return route_boxes(img, boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b0389e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ JSON‚Äë–≤—ã–≤–æ–¥–∞ –≤ Markdown / LaTeX\n",
    "def json_to_markdown(result: List[Dict]) -> str:\n",
    "    '''\n",
    "    –°–æ—Ä—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç—ã —Å–≤–µ—Ä—Ö—É‚Äë–≤–Ω–∏–∑ —Å–ª–µ–≤–∞‚Äë–Ω–∞–ø—Ä–∞–≤–æ –∏ –≤—Å—Ç–∞–≤–ª—è–µ–º\n",
    "    —Ñ–æ—Ä–º—É–ª—ã –∫–∞–∫ $$ ... $$, –æ—Å—Ç–∞–ª—å–Ω–æ–µ ‚Äî –æ–±—ã—á–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º.\n",
    "    '''\n",
    "    md_lines = []\n",
    "    for obj in sorted(result, key=lambda o: (o['bbox'][1], o['bbox'][0])):\n",
    "        if obj['type'] == 'formula':\n",
    "            md_lines.append(f'$$\\n{obj[\"text\"]}\\n$$')\n",
    "        else:\n",
    "            md_lines.append(obj['text'])\n",
    "    return '\\n\\n'.join(md_lines)\n",
    "\n",
    "def save_outputs(result: List[Dict], stem: str = 'page'):\n",
    "    stem = Path(stem)\n",
    "    stem.with_suffix('.json').write_text(json.dumps(result, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "    md = json_to_markdown(result)\n",
    "    stem.with_suffix('.md').write_text(md,  encoding='utf-8')\n",
    "    stem.with_suffix('.tex').write_text(md, encoding='utf-8')\n",
    "    print('–§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:', stem.with_suffix('.json'), stem.with_suffix('.md'), stem.with_suffix('.tex'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "afbcbfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: results/page.json results/page.md results/page.tex\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'bbox': [184, 20, 220, 42], 'type': 'text', 'text': '–∞ 1.', 'conf': 0.99},\n",
       " {'bbox': [221, 21, 251, 42],\n",
       "  'type': 'formula',\n",
       "  'text': 'a_{-1}',\n",
       "  'conf': 0.99},\n",
       " {'bbox': [98, 20, 179, 48], 'type': 'text', 'text': '—Ä–µ–º–∞ 1', 'conf': 0.99}]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "result = process_image(\"../../../test/cimage01.png\")\n",
    "save_outputs(result, \"results/page\")\n",
    "result[:5]          # –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø–µ—Ä–≤—ã–µ 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5b65d5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--out OUT] image\n",
      "ipykernel_launcher.py: error: the following arguments are required: image\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/legendwp/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# %% –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ã (—Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –≤ .py)\n",
    "if __name__ == '__main__':\n",
    "    import argparse, pprint\n",
    "    parser = argparse.ArgumentParser(description='Split image into text + formula boxes and OCR them.')\n",
    "    parser.add_argument('image', help='path to jpg / png with formulas + handwritten text')\n",
    "    parser.add_argument('--out',  help='stem for output files (without extension)', default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    out_stem = args.out or Path(args.image).with_suffix('')\n",
    "    res = process_image(args.image)\n",
    "    save_outputs(res, out_stem)\n",
    "    pprint.pprint(res[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
